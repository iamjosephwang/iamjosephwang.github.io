<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>COMPSCI5088 final revision notebook | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="week1 lecture01-01 What is big data1.backgroundData whose capturing, storage, curation, management, and processing are not possible with traditional tool within tolerable amount of time.  Defined by">
<meta property="og:type" content="article">
<meta property="og:title" content="COMPSCI5088 final revision notebook">
<meta property="og:url" content="http://example.com/2024/03/18/COMPSCI5088-final-revision-notebook/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="week1 lecture01-01 What is big data1.backgroundData whose capturing, storage, curation, management, and processing are not possible with traditional tool within tolerable amount of time.  Defined by">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/BD_week1_usecase00.png">
<meta property="og:image" content="http://example.com/img/BD_week2_master_service.png">
<meta property="og:image" content="http://example.com/img/BD_week2_MapReduce_count_example.png">
<meta property="og:image" content="http://example.com/img/BD_week2_MapReduce_read_write.png">
<meta property="og:image" content="http://example.com/img/BD_week2_Hadoop_architecture.png">
<meta property="og:image" content="http://example.com/img/BD_week2_move_compute_to_data.png">
<meta property="og:image" content="http://example.com/img/BD_week_Hadoop_work_distribution_challenge.png">
<meta property="og:image" content="http://example.com/img/BD_week3_Hadoop_Failure_types.png">
<meta property="og:image" content="http://example.com/img/BD_week5_DFS_N_MNode.png">
<meta property="og:image" content="http://example.com/img/BD_week5_Hadoop_D_N.png">
<meta property="article:published_time" content="2024-03-18T21:52:25.000Z">
<meta property="article:modified_time" content="2024-04-04T12:09:35.496Z">
<meta property="article:author" content="Joseph Wang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/BD_week1_usecase00.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.0.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="posts-COMPSCI5088-final-revision-notebook" class="h-entry article article-type-posts" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/18/COMPSCI5088-final-revision-notebook/" class="article-date">
  <time class="dt-published" datetime="2024-03-18T21:52:25.000Z" itemprop="datePublished">2024-03-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      COMPSCI5088 final revision notebook
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <hr>
<h2 id="week1-lecture"><a href="#week1-lecture" class="headerlink" title="week1 lecture"></a>week1 lecture</h2><h3 id="01-01-What-is-big-data"><a href="#01-01-What-is-big-data" class="headerlink" title="01-01 What is big data"></a>01-01 What is big data</h3><h4 id="1-background"><a href="#1-background" class="headerlink" title="1.background"></a>1.background</h4><p>Data whose capturing, storage, curation, management, and processing are not possible with traditional tool within tolerable amount of time.</p>
<ol>
<li>Defined by Use:<br> “<strong>capturing, storage, curation, management</strong>“ - refers to how we store and access it.</li>
</ol>
<p> “<strong>processing</strong>“ how we can use it to gain value</p>
<ol start="2">
<li>by the Software and Hardware available to us<br> “traditional tools”: Software<br> “tolerable amount of time” - refers to ‘<strong>What hardware we have to run that software</strong>‘</li>
</ol>
<h4 id="2-Dimension-of-Big-Data"><a href="#2-Dimension-of-Big-Data" class="headerlink" title="2.Dimension of Big Data"></a>2.Dimension of Big Data</h4><ol>
<li><p>Volume</p>
</li>
<li><p>Variety</p>
</li>
<li><p>Veracity: refers to the reliability and quality of data.</p>
</li>
<li><p>Velocity: Describes the speed at which data is generated and processed.</p>
</li>
<li><p>Variability:</p>
</li>
</ol>
<span id="more"></span>
<h3 id="01-02-Hardware-Primer"><a href="#01-02-Hardware-Primer" class="headerlink" title="01-02 Hardware Primer"></a>01-02 Hardware Primer</h3><h4 id="1-Four-aspects-we-need-to-know-about-hardware"><a href="#1-Four-aspects-we-need-to-know-about-hardware" class="headerlink" title="1. Four aspects we need to know about hardware"></a>1. Four aspects we need to know about hardware</h4><h4 id="1-1-General-Purpose-Compute"><a href="#1-1-General-Purpose-Compute" class="headerlink" title="1.1 General Purpose Compute"></a>1.1 General Purpose Compute</h4><ul>
<li>How quickly the CPU can complete a given task</li>
<li>CPUs become faster:</li>
</ul>
<ol>
<li>more transistors</li>
<li>more optimal internal integrated architecture of these transistor for common operations</li>
</ol>
<ul>
<li>Representative benchmarks to evaluate compute capacity</li>
</ul>
<ol>
<li>those benchmarks provide a suite of synthetic workloads that stress various aspects of CPU performance</li>
<li>including arithmetic processing power, integer and floating-point operations, memory access speed, and overall throughput.</li>
</ol>
<h4 id="1-2-Specialist-Compute"><a href="#1-2-Specialist-Compute" class="headerlink" title="1.2 Specialist Compute"></a>1.2 Specialist Compute</h4><ul>
<li>Specialist hardware accelerators or add-ons to speed up compute<br> examples like:</li>
</ul>
<ol>
<li>GPUs for DL</li>
<li>Quantum Cores</li>
</ol>
<h4 id="1-3-Storage-Stack"><a href="#1-3-Storage-Stack" class="headerlink" title="1.3 Storage Stack"></a>1.3 Storage Stack</h4><p>commonplace for the tiered architecture of storage</p>
<ul>
<li>Register</li>
<li>Cache</li>
<li>Main Memory</li>
<li>Solid State Drive</li>
<li>Magnetic Drives</li>
</ul>
<h4 id="1-4-Network"><a href="#1-4-Network" class="headerlink" title="1.4 Network"></a>1.4 Network</h4><ul>
<li><p>distribution of work across multiple machine</p>
</li>
<li><p>machines need to communicate, incurring transfer latencies across the LAN</p>
</li>
<li><p>individual machines are physically connected to central routers&#x2F;switches</p>
</li>
</ul>
<h3 id="01-03-User-Needs"><a href="#01-03-User-Needs" class="headerlink" title="01-03 User Needs"></a>01-03 User Needs</h3><h4 id="1-Three-main-Big-data-use-cases"><a href="#1-Three-main-Big-data-use-cases" class="headerlink" title="1. Three main Big data use-cases"></a>1. Three main Big data use-cases</h4><ol>
<li>Extraction&#x2F;Filtering</li>
<li>Transformation</li>
<li>Aggregation - refers to merge a set of data items</li>
</ol>
<h4 id="2-Batch-vs-Streaming"><a href="#2-Batch-vs-Streaming" class="headerlink" title="2. Batch vs. Streaming"></a>2. Batch vs. Streaming</h4><p>consider whether we are working with largely static data, or new adta is arriving over time</p>
<ul>
<li><p>Static Data: Tackled with batch processing, where large blocks of that data are analysed at once, usually with multiple blocks being processed concurrently, that are subsequently merged together.<br> Cares about completetion time</p>
</li>
<li><p>Stream Data: Tackled with Stream processing, where items are processed individually within a pipeline of stages, where multiple items can be processed concurrently within the pipeline<br> Cares bout processing latency per items</p>
</li>
<li><p>Stream Processing</p>
</li>
</ul>
<ol>
<li>Continuous data streams:</li>
</ol>
<ul>
<li>Stream processing deals with data that arrives continuously and may have no inherent end</li>
<li>data stream can come from various sources</li>
</ul>
<ol start="2">
<li>Real-time Processing:</li>
</ol>
<ul>
<li>aims to process data in real-time  or with minimal latency</li>
<li>enables organization to react swiftly to changing conditions, make timely decisions, and take immediate actions.</li>
</ul>
<ol start="3">
<li>Event-Driven Architecture:</li>
</ol>
<ul>
<li>built on event-driven architecture, where events trigger actions or processes.</li>
<li>Events represent meaningful occurences or changes in the data stream.</li>
</ul>
<ol start="4">
<li>Stateful Processing</li>
</ol>
<ul>
<li>Stream processing may involve maintaining stateful computations over time, where the processing logic depends on the history of events.</li>
<li>Stateful processing allows for more sophisticated analysis and pattern recognition</li>
</ul>
<ol start="5">
<li>Scalability and Fault Tolerance:</li>
</ol>
<ul>
<li>Stream processing systems must be scalable to handle high-volume data streams efficiently.</li>
<li>They should also be fault-tolerant, capable of recovering from failures and ensuring that no data is lost or duplicated during processing.</li>
</ul>
<p><img src="/../img/BD_week1_usecase00.png" alt="Examples about use-cases"></p>
<p>Real-world application of Stream Filtering:</p>
<ol>
<li>Anomaly Detection</li>
<li>Content Moderation</li>
<li>Data Quality Assurance</li>
<li>Fraud detection</li>
</ol>
<h3 id="01-04-Scaling-Vertical-vs-Horizontal-Scaling"><a href="#01-04-Scaling-Vertical-vs-Horizontal-Scaling" class="headerlink" title="01-04 Scaling - Vertical vs. Horizontal Scaling"></a>01-04 Scaling - Vertical vs. Horizontal Scaling</h3><p>traditional solution to Big Data problems, like leveraging more or faster hardware, has physical limits.</p>
<p>Horizontal Scaling: distribute work over a cluster comprised of many low cost machines</p>
<h3 id="01-05-Compute-Parallelism"><a href="#01-05-Compute-Parallelism" class="headerlink" title="01-05 Compute Parallelism"></a>01-05 Compute Parallelism</h3><ul>
<li><p>Parallel Computing: Solve big data problems by breaking down the problem into multiple tasks that can be solved simultaneously</p>
</li>
<li><p>Examples: Server Meshing used in MMOs<br> powerful technique for managing the complexity of multiplayer online games, enabling seamless interactions between players and creating immersive and dynamic virtual worlds</p>
</li>
</ul>
<ol>
<li>Dynamic Partitioning:</li>
<li>Seamless Transition:</li>
<li>Load Balancing<br> use keyword ‘server meshing’ to find more detailed information in my chatbar.</li>
</ol>
<h4 id="1-Where-can-compute-parallelism-happen"><a href="#1-Where-can-compute-parallelism-happen" class="headerlink" title="1. Where can compute parallelism happen?"></a>1. Where can compute parallelism happen?</h4><ol>
<li>Within a CPU core:<br> can process multiple instructions simultaneously due to pipeline and superscalar support</li>
</ol>
<ul>
<li><p>Pipelining: split the processor datapath into multiple successive stages with instructions moving through each stage sequentially.</p>
</li>
<li><p>Superscalar: Processors that house multiple copies of certain commonly used components.(arithmetic logic units)</p>
</li>
</ul>
<ol start="2">
<li>Across CPU cores:</li>
</ol>
<ul>
<li>exposed to programmers as ‘threads’</li>
</ul>
<ol start="3">
<li>Across CPU cores in different machines</li>
</ol>
<ul>
<li>Tasks are typically compiled and then sent to a machine with some data to execute on that task</li>
</ul>
<h4 id="2-Hardware-classification-for-CPUs"><a href="#2-Hardware-classification-for-CPUs" class="headerlink" title="2. Hardware classification for CPUs"></a>2. Hardware classification for CPUs</h4><ul>
<li><p><strong>SISD</strong> - serial computer</p>
</li>
<li><p><strong>MIMD</strong> - collections of autonomous processors that can execute multiple independent programs, each of which can have its own data stream. modern CPUs</p>
</li>
<li><p><strong>SIMD</strong> - Data is divided among the processors and each data item is subjected to the same sequence of instructions. </p>
</li>
<li><p><strong>MISD</strong> - rare</p>
</li>
</ul>
<h4 id="3-use-speed-up-linear-speed-up-to-evaluate-the-parallelism-effectiveness"><a href="#3-use-speed-up-linear-speed-up-to-evaluate-the-parallelism-effectiveness" class="headerlink" title="3. use speed-up, linear speed-up to evaluate the parallelism effectiveness"></a>3. use speed-up, linear speed-up to evaluate the parallelism effectiveness</h4><ul>
<li>parallelism time &#x2F; serial time</li>
</ul>
<h2 id="week2-lecture"><a href="#week2-lecture" class="headerlink" title="week2 lecture"></a>week2 lecture</h2><h3 id="02-01-Compute-Callenges"><a href="#02-01-Compute-Callenges" class="headerlink" title="02-01 Compute Callenges"></a>02-01 Compute Callenges</h3><h4 id="1-Data-spliting-and-Load-Balancing"><a href="#1-Data-spliting-and-Load-Balancing" class="headerlink" title="1. Data spliting and Load Balancing"></a>1. Data spliting and Load Balancing</h4><ul>
<li><p>To enable parallelism, we need to split our data into n chunks. n tend to be greater than the number of parallel ‘pipelines’ we have</p>
</li>
<li><p>to make effective use of resources, we want each pipeline to have the same amount of work to do.</p>
</li>
</ul>
<h4 id="2-Communication-and-Intermediate-Storage-overheads"><a href="#2-Communication-and-Intermediate-Storage-overheads" class="headerlink" title="2. Communication and Intermediate Storage overheads"></a>2. Communication and Intermediate Storage overheads</h4><p>As we add more tasks and stages to our system, we have more communication links between those tasks<br> whether it could be a problems depends on how data transfer between tasks</p>
<ul>
<li>if both tasks are hosted on the same physical machine, the cost of this will depend on storage speed </li>
<li>if on different machines, we should consider to add network transfer time</li>
</ul>
<h4 id="3-Master-service"><a href="#3-Master-service" class="headerlink" title="3. Master service"></a>3. Master service</h4><ul>
<li>we need software to create the tasks</li>
<li>we call these master services</li>
<li>A master service is responsible for allocating tasks to machine<br> physical machine has a number of slots on it, which can be plugged into tasks.</li>
</ul>
<p> <img src="/../img/BD_week2_master_service.png" alt="Master Service"><br>need to consider:</p>
<ol>
<li>Fault tolerance<br> What happens if machine fails to mid processing?</li>
</ol>
<p> We should ideally re-allocate the work to a new machine<br> refers to:</p>
<ol>
<li><p>get a fresh copy of the data to be processed</p>
</li>
<li><p>restart the tasks<br> orchestrated by the master service. </p>
</li>
<li><p>master failure</p>
</li>
<li><p>Network failure</p>
</li>
</ol>
<h3 id="02-02-MapReduce"><a href="#02-02-MapReduce" class="headerlink" title="02-02 MapReduce"></a>02-02 MapReduce</h3><p>Why work distribution is challenging:</p>
<ul>
<li>overheads</li>
<li>load balancing</li>
<li>orchestration</li>
</ul>
<p>solution to all of these problems known as Mapreduce</p>
<p><a target="_blank" rel="noopener" href="https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf">Simplified Data Processing on large clusters</a></p>
<p><img src="/../img/BD_week2_MapReduce_count_example.png" alt="MapReduce for counting example"></p>
<p>Note that in map step, we generally gather the data using a brand new key, and for each key generate the corresponding lists, that is we do data extraction,filtering, and transformation in this stage.</p>
<p>‘’’java<br>    &lt;key1, value1&gt; -&gt; &lt;key2, List <value2>&gt;<br>    &lt;key1, value1&gt; -&gt; &lt;key2, value2&gt;  </p>
<p>‘’’</p>
<p>In reduce step, we need to collect values or value lists for separate key.<br>That is we do aggregation in this stage.<br>‘’’java<br>    &lt;key2, List<value2>&gt; -&gt; value3<br>    &lt;key2, List<value2>&gt; -&gt; List<value3></p>
<p>‘’’</p>
<p>MapReduce: write and read<br>data gets re-organized and then stored in intermediate stage.<br><img src="/../img/BD_week2_MapReduce_read_write.png" alt="intermediate storage:read and write"></p>
<h3 id="02-03-Hadoop-basics"><a href="#02-03-Hadoop-basics" class="headerlink" title="02-03 Hadoop basics"></a>02-03 Hadoop basics</h3><h4 id="1-Hadoop-architecture"><a href="#1-Hadoop-architecture" class="headerlink" title="1. Hadoop architecture"></a>1. Hadoop architecture</h4><h5 id="1-1-Hadoop-Compute"><a href="#1-1-Hadoop-Compute" class="headerlink" title="1.1 Hadoop Compute"></a>1.1 Hadoop Compute</h5><ul>
<li><p><strong>Client:</strong> users use to sumbit work to the Hadoop cluster, can communicate with the Job tracker</p>
</li>
<li><p><strong>Job Tracker:</strong> Receives work from the client, uses the Name Node to identify where data is located, assigns work to task trackers</p>
</li>
<li><p><strong>Task tracker:</strong> Execute Map and Reduce tasks</p>
</li>
</ul>
<h5 id="1-2-Hadoop-HDFS"><a href="#1-2-Hadoop-HDFS" class="headerlink" title="1.2 Hadoop HDFS"></a>1.2 Hadoop HDFS</h5><ul>
<li><p><strong>NameNode:</strong> maintains metadata, tracks the location of all resources on HDFS, and handls replication.</p>
</li>
<li><p><strong>Data Node:</strong> handles storage and access of the local files on the machine.</p>
</li>
</ul>
<p><img src="/../img/BD_week2_Hadoop_architecture.png" alt="Hadoop and HDFS architecture"></p>
<h4 id="2-Overheads-in-Hadoop-Mapreduce"><a href="#2-Overheads-in-Hadoop-Mapreduce" class="headerlink" title="2.Overheads in Hadoop Mapreduce"></a>2.Overheads in Hadoop Mapreduce</h4><p>HDFS read before ‘Reduce’ stage inloves Network latency as data gets shuffled accross machines.</p>
<h4 id="3-Data-Locality"><a href="#3-Data-Locality" class="headerlink" title="3.Data Locality"></a>3.Data Locality</h4><p>move the compute to data</p>
<p><img src="/../img/BD_week2_move_compute_to_data.png" alt="load compute to data"></p>
<h4 id="4-Work-distribution-challenges"><a href="#4-Work-distribution-challenges" class="headerlink" title="4.Work distribution challenges"></a>4.Work distribution challenges</h4><p>almost same as Mapreduce</p>
<p><img src="/../img/BD_week_Hadoop_work_distribution_challenge.png" alt="work distribution challenges w.r.t Hadoop"></p>
<h2 id="week3-Lecture"><a href="#week3-Lecture" class="headerlink" title="week3 Lecture"></a>week3 Lecture</h2><h3 id="03-01-Hadoop-Execution"><a href="#03-01-Hadoop-Execution" class="headerlink" title="03-01 Hadoop Execution"></a>03-01 Hadoop Execution</h3><h4 id="1-Execution-streamline"><a href="#1-Execution-streamline" class="headerlink" title="1.Execution streamline"></a>1.Execution streamline</h4><p>for the detailed informaton about execution in Hadoop, just scan through the slides.</p>
<h4 id="2-Input-splitting"><a href="#2-Input-splitting" class="headerlink" title="2.Input splitting"></a>2.Input splitting</h4><h4 id="3-Task-Assignment"><a href="#3-Task-Assignment" class="headerlink" title="3.Task Assignment"></a>3.Task Assignment</h4><p><strong>speculative execution:</strong></p>
<p>idle task trackers are identified using Job tracker-task tracker heartbeats.</p>
<ul>
<li><p>If the JobTracker detects that a task is progressing significantly slower than other similar tasks (e.g., due to hardware issues, resource contention, or other factors), it marks that task as a candidate for speculative execution.</p>
</li>
<li><p>the JobTracker launches a duplicate or speculative task on another node in the cluster.</p>
</li>
<li><p>The speculative task performs the same computation as the original task but runs independently and in parallel with the original task.</p>
</li>
</ul>
<p><strong>Note:</strong> use keyword ‘speculative task’ to fetch more details in my chatbar.</p>
<h3 id="03-02-Hadoop-extras"><a href="#03-02-Hadoop-extras" class="headerlink" title="03-02 Hadoop extras"></a>03-02 Hadoop extras</h3><h4 id="1-Distributed-cache"><a href="#1-Distributed-cache" class="headerlink" title="1. Distributed cache"></a>1. Distributed cache</h4><p>Small files can be an issue, as HDFS can only read full blocks at a time.</p>
<p><strong>Aternative method:</strong> use distributed cache to share files between job trackers and task trackers</p>
<ul>
<li><p>Read-Only</p>
</li>
<li><p>can be used for jars, text, archives, conf files, etc</p>
</li>
<li><p>jars, lib can be added to the task’s classpath</p>
</li>
<li><p>Files are copied only once per job to each task tracker</p>
</li>
<li><p>can be used as a ‘hack’ to share state across tasks</p>
</li>
<li><p>can be private or public</p>
</li>
</ul>
<h4 id="2-Counters"><a href="#2-Counters" class="headerlink" title="2. Counters"></a>2. Counters</h4><p>we would liket to track the progress of tasks, and Hadoop allows tasks to create and update numerical counters at run-time that are periodically shared with the Job trackers.</p>
<p>support both built-in and user-defined counters.</p>
<h4 id="3-Compression"><a href="#3-Compression" class="headerlink" title="3.Compression"></a>3.Compression</h4><p>Due to the high cost of read&#x2F;write w.r.t HDFS, it is highly recommended that data is compressed.</p>
<ul>
<li><p>Less I&#x2F;Os across the dataset contributes to higher throughput.</p>
</li>
<li><p>More data per HDFS block&#x2F; partition contributs to lower storage&#x2F;network overhead</p>
</li>
<li><p>incur additional CPU cost for decompression.</p>
</li>
</ul>
<h4 id="4-Complex-failure-types"><a href="#4-Complex-failure-types" class="headerlink" title="4. Complex failure types"></a>4. Complex failure types</h4><p><img src="/../img/BD_week3_Hadoop_Failure_types.png" alt="more on Hadoop failure types"></p>
<h3 id="03-03-Issues-with-Hadoops"><a href="#03-03-Issues-with-Hadoops" class="headerlink" title="03-03 Issues with Hadoops"></a>03-03 Issues with Hadoops</h3><h4 id="1-Small-files"><a href="#1-Small-files" class="headerlink" title="1. Small files"></a>1. Small files</h4><p>each file will be assigned with a separate block entry in the name node, which means the name node index would be overloaded if we have very large numbers of files.</p>
<p>can be overcome using:</p>
<ul>
<li>merging small files together</li>
<li>using HAR files</li>
<li>using Hbase, we’ll cover this in week8 lecture (08-01 Big Table and 08-02 HBase)</li>
</ul>
<h4 id="2-Speed-and-Latency"><a href="#2-Speed-and-Latency" class="headerlink" title="2. Speed and Latency"></a>2. Speed and Latency</h4><p>due to small and expensive main memory and no caching</p>
<h4 id="3-Cost-of-Chaining-Map-and-Reduce"><a href="#3-Cost-of-Chaining-Map-and-Reduce" class="headerlink" title="3. Cost of Chaining Map and Reduce"></a>3. Cost of Chaining Map and Reduce</h4><p>Hadoop developers did not actually make it easy or fast to chain MapReduce pairs in sequence. In that case, data needs to be written to disk and then re-read between each link in the chain.</p>
<h4 id="other-Use-Cases-cannot-be-covered-by-Hadoop"><a href="#other-Use-Cases-cannot-be-covered-by-Hadoop" class="headerlink" title="other Use-Cases cannot be covered by Hadoop"></a>other Use-Cases cannot be covered by Hadoop</h4><ul>
<li><p>applications that need to share state across instances of a task</p>
</li>
<li><p>All streaming use-cases</p>
</li>
<li><p>Appllication with significant side-affects????</p>
</li>
</ul>
<h2 id="week5-Lecture"><a href="#week5-Lecture" class="headerlink" title="week5 Lecture"></a>week5 Lecture</h2><h3 id="05-02-Distributed-Storage"><a href="#05-02-Distributed-Storage" class="headerlink" title="05-02 Distributed Storage"></a>05-02 Distributed Storage</h3><p>What do you expect of a file system?</p>
<ol>
<li>a namespace(filenames, directory names, etc)</li>
<li>format(How the files are actually organized)</li>
<li>index(mappings from filenames to physical address)</li>
<li>read&#x2F;write path and API implementation</li>
</ol>
<p>What is distributed data<br>Distributed storage exists for the same reason that distributed compute exists.</p>
<ul>
<li>need more hardware capacity</li>
<li>would like to distribute our available storage media across physical machines. </li>
<li>most bd app are performed via the distributed file system.</li>
</ul>
<p>Motivation for the distributed storage.</p>
<ul>
<li><p>Resiliency</p>
</li>
<li><p>Speed</p>
</li>
<li><p>Scalability: Like ‘horizontal scaling’ in compute, we can theoretically infinitely scale storage in a horizontal manner.</p>
</li>
<li><p>Data Locality: if data is replicated to multiple locations, then we will often be able to schedule<br>work close to that data</p>
</li>
</ul>
<p>No best solution for distributed storage, just make a trade-off between:</p>
<ul>
<li>Reliability</li>
<li>Availability</li>
<li>Performance</li>
<li>Capacity</li>
</ul>
<p>OneDrive Example: all the issues and ways this can go wrong.</p>
<h3 id="05-03-NFS"><a href="#05-03-NFS" class="headerlink" title="05-03 NFS"></a>05-03 NFS</h3><p>follows a client-server architecture</p>
<ul>
<li>Servers keep the files</li>
<li>clients read&#x2F;write the files.</li>
</ul>
<p>most operating systems have <strong>software file management layer</strong></p>
<ul>
<li>allows for read&#x2F;write process from devices of different format.</li>
<li>Virtual File System</li>
<li>allows clients to mirror file directory held on the other machines.</li>
</ul>
<p>NFS server side:</p>
<ul>
<li><strong>stateless:</strong></li>
<li><strong>Locking:</strong></li>
</ul>
<p><strong>Mounting</strong><br>The user can request that an NFS client mirror a specified remote directory exposed by an NFS server</p>
<p>NFS Mounting is a multi-stage process</p>
<ul>
<li><p>The client first requests the mount port number from the portmap service on the NFS server</p>
</li>
<li><p>The client pings the specified port on the server to make sure its live</p>
</li>
<li><p>The client then makes a mount request to the server, getting the file handle for the file system</p>
</li>
<li><p>The client connects to the port using the file handle and requests the information about the file system</p>
</li>
<li><p>Multiple machines can mount the same file system<br> Parallel reads allowed, but not parallel writes</p>
</li>
</ul>
<h3 id="05-04-HDFS"><a href="#05-04-HDFS" class="headerlink" title="05-04 HDFS"></a>05-04 HDFS</h3><h4 id="1-Recap-for-the-Hadoop-architecture"><a href="#1-Recap-for-the-Hadoop-architecture" class="headerlink" title="1.Recap for the Hadoop architecture"></a>1.Recap for the Hadoop architecture</h4><ol>
<li><strong>Client</strong></li>
<li><strong>Job Tracker</strong></li>
<li><strong>Task Tracker</strong></li>
<li><strong>Name Node</strong></li>
<li><strong>Data Node</strong></li>
</ol>
<h4 id="2-an-HDFS-file-is-actually-divided-into-one-or-more-blocks"><a href="#2-an-HDFS-file-is-actually-divided-into-one-or-more-blocks" class="headerlink" title="2.an HDFS file is actually divided into one or more blocks"></a>2.an HDFS file is actually divided into one or more blocks</h4><p>blocks are replicated amongst machines(across multiple data nodes).</p>
<h4 id="3-Architectural-principles"><a href="#3-Architectural-principles" class="headerlink" title="3.Architectural principles"></a>3.Architectural principles</h4><ol>
<li>Unix-like File system API</li>
<li>Unix-like index nodes(describes the metadata for files and directories)</li>
<li>Unix-like file paths</li>
<li>Separates file content data from metadata(DN and NN)</li>
</ol>
<h4 id="4-Name-Node-Master-Node"><a href="#4-Name-Node-Master-Node" class="headerlink" title="4.Name Node&#x2F;Master Node"></a>4.Name Node&#x2F;Master Node</h4><p><img src="/../img/BD_week5_DFS_N_MNode.png" alt="Name Node/Master Node"></p>
<ul>
<li>About Write-Ahead-Log<br> <strong>How it works?</strong></li>
</ul>
<ol>
<li><p>Logging Changes: When data modifications are made (e.g., inserts, updates, deletes), the changes are first written to the Write-Ahead Log before being applied to the main data storage.</p>
</li>
<li><p>Sequential Writes: Write-Ahead Logs are typically append-only files, meaning that new changes are appended to the end of the log in sequential order. This sequential write pattern optimizes disk I&#x2F;O performance.</p>
</li>
<li><p>Durability Guarantees: By logging changes to a Write-Ahead Log before applying them to the main storage system, Hadoop ensures durability guarantees. Even if the system crashes or experiences a failure before changes are fully applied, the logged changes can be replayed from the Write-Ahead Log during recovery to restore the system to a consistent state.</p>
</li>
<li><p>Recovery Process: During system recovery or restart, Hadoop components can replay the logged changes from the Write-Ahead Log to bring the system back to a consistent state. This process helps prevent data loss and ensures that no committed changes are lost due to failures.</p>
</li>
</ol>
<p> <strong>Benefits</strong></p>
<ol>
<li>Durability:</li>
<li>Fault Tolerance</li>
<li>Consistency</li>
<li>Performance</li>
</ol>
<h4 id="5-Data-Node"><a href="#5-Data-Node" class="headerlink" title="5. Data Node"></a>5. Data Node</h4><p><img src="/../img/BD_week5_Hadoop_D_N.png" alt="Data Node"></p>
<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/03/18/COMPSCI5088-final-revision-notebook/" data-id="cltyis1o30000qsfw75ml91ct" data-title="COMPSCI5088 final revision notebook" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2024/02/04/learn-CPP-storage-from-scratch/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">learn CPP storage from scratch</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Programming-2023tutorial/">-Programming -2023tutorial</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/c-Programming/" rel="tag">-c++ -Programming</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/deep-learning/" rel="tag">-deep learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm-Leetcode-Graph-Problems/" rel="tag">Algorithm, Leetcode, Graph Problems</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/COMPSCI4084/" rel="tag">COMPSCI4084</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Introduction-to-data-science-and-system/" rel="tag">Introduction to data science and system</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/learning-from-scratch/" rel="tag">learning from scratch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/" rel="tag">machine learning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/c-Programming/" style="font-size: 20px;">-c++ -Programming</a> <a href="/tags/deep-learning/" style="font-size: 10px;">-deep learning</a> <a href="/tags/Algorithm-Leetcode-Graph-Problems/" style="font-size: 20px;">Algorithm, Leetcode, Graph Problems</a> <a href="/tags/COMPSCI4084/" style="font-size: 10px;">COMPSCI4084</a> <a href="/tags/Introduction-to-data-science-and-system/" style="font-size: 10px;">Introduction to data science and system</a> <a href="/tags/Machine-Learning/" style="font-size: 10px;">Machine Learning</a> <a href="/tags/learning-from-scratch/" style="font-size: 10px;">learning from scratch</a> <a href="/tags/machine-learning/" style="font-size: 20px;">machine learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">February 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">September 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">February 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/03/18/COMPSCI5088-final-revision-notebook/">COMPSCI5088 final revision notebook</a>
          </li>
        
          <li>
            <a href="/2024/02/04/learn-CPP-storage-from-scratch/">learn CPP storage from scratch</a>
          </li>
        
          <li>
            <a href="/2024/01/13/%E9%9D%A2%E7%BB%8F%E6%B3%A8%E8%A7%A3/">面经注解</a>
          </li>
        
          <li>
            <a href="/2023/09/29/numpy-learning-from-scratch/">numpy learning from scratch</a>
          </li>
        
          <li>
            <a href="/2022/09/15/Use-Kruskal-Prim-to-solve-Min-cost-to-connect-all-points-problem/">Use Kruskal &amp; Prim to solve &#39;Min cost to connect all points&#39; problem</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 Joseph Wang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>