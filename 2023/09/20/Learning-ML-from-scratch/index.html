<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Learning ML from scratch | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Unit1 Regression2.Polynomial Regressionoverview of this part  More general form of linear regression - implement our linear model in vector form More general formulation Making predictions Genera">
<meta property="og:type" content="article">
<meta property="og:title" content="Learning ML from scratch">
<meta property="og:url" content="http://example.com/2023/09/20/Learning-ML-from-scratch/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Unit1 Regression2.Polynomial Regressionoverview of this part  More general form of linear regression - implement our linear model in vector form More general formulation Making predictions Genera">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-20T19:23:30.000Z">
<meta property="article:modified_time" content="2023-11-19T23:08:42.996Z">
<meta property="article:author" content="Joseph Wang">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.0.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="custom-layout-Learning-ML-from-scratch" class="h-entry article article-type-custom-layout" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/20/Learning-ML-from-scratch/" class="article-date">
  <time class="dt-published" datetime="2023-09-20T19:23:30.000Z" itemprop="datePublished">2023-09-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Learning ML from scratch
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- Add the MathJax script within your blog post -->
<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<hr>
<h2 id="Unit1-Regression"><a href="#Unit1-Regression" class="headerlink" title="Unit1 Regression"></a>Unit1 Regression</h2><h3 id="2-Polynomial-Regression"><a href="#2-Polynomial-Regression" class="headerlink" title="2.Polynomial Regression"></a>2.Polynomial Regression</h3><p>overview of this part</p>
<ol>
<li>More general form of linear regression - implement our linear model in vector form</li>
<li>More general formulation</li>
<li>Making predictions</li>
<li>Generalisation, overfitting, cross-validation<span id="more"></span></li>
</ol>
<h4 id="2-1-workflows-of-implement-linear-regresion"><a href="#2-1-workflows-of-implement-linear-regresion" class="headerlink" title="2.1 workflows of implement linear regresion"></a>2.1 workflows of implement linear regresion</h4><ol>
<li>Set up Simple model:<br> $$t &#x3D; w_{0} + w_{1}x$$</li>
<li>Take partial deriative of loss function w.r.t each parameter</li>
<li>Set deriatives to zero and solve the equations</li>
</ol>
<p>Now we have more complicated model:</p>
<ol>
<li>more complex expression<br>$$<br>t &#x3D; w_{0} + w_{1}x + w_{2}x^{2} + w_{3}x^{3} + \ldots + w_{K}x^{K} &#x3D; \sum_{k&#x3D;0}^{K}w_{k}x^{k}<br>$$</li>
</ol>
<p> you’re high recommended to incapsulate parmaters in a vector<br> firstly, make trasnformation towards your data.<br> your original data(label x) maybe a N-Dimensional array.<br> after transformation, it’ll be something like below:<br>$$<br>\mathbf{X} &#x3D; \begin{bmatrix}<br>1 &amp; x_{1} &amp; x_{1}^2 &amp; \dots &amp; x_{1}^K<br>\newline<br>1 &amp; x_{2} &amp; x_{2}^2 &amp; \dots &amp; x_{2}^K<br>\newline<br>\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots<br>\newline<br>1 &amp; x_{N} &amp; x_{N}^2 &amp; \dots &amp; x_{N}^K<br>\end{bmatrix}<br>$$<br> by the way, you can find one version of achieving this implementation in my<a target="_blank" rel="noopener" href="https://iamjosephwang.github.io/2023/10/15/My-solution-for-UofG-23Fall-COMPSCI5100-week2-lab/">lab2 solution - make_polynomial</a><br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_polynomial</span>(<span class="params">x, maxorder</span>): <span class="comment"># The np.hstack function can be very helpful</span></span><br><span class="line">   X = np.ones_like(x) </span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,maxorder+<span class="number">1</span>):</span><br><span class="line">       X = np.hstack((X,x**i))</span><br><span class="line">   <span class="keyword">return</span>(X) </span><br></pre></td></tr></table></figure><br> more general models:<br> In fact, each term w.r.t x can be any function of x<br>$$<br>    t&#x3D;w_{0}h_{0}(x) + w_{1}h_{1}(x)+\dot+w_{k}h_{k}(x)<br>$$<br> In general, we can describe our function form of x in a matrix way below:<br>$$<br>\mathbf{X} &#x3D; \begin{bmatrix}<br>h_{0}(x_{1}) &amp; h_{1}(x_{1}) &amp; \dots &amp; h_{K}(x_{1})<br>\newline<br>h_{0}(x_{2}) &amp; h_{1}(x_{2}) &amp; \dots &amp; h_{K}(x_{2})<br>\newline<br>\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots<br>\newline<br>h_{0}(x_{N}) &amp; h_{1}(x_{N}) &amp; \dots &amp; h_{K}(x_{N})<br>\end{bmatrix}<br>$$</p>
<ol start="2">
<li>Define mean loss function $$L&#x3D;\frac{1}{N}\sum_{n&#x3D;1}^{N}(t_{n} - \sum_{k&#x3D;0}^{K}w_{k}x_{n}^{k})^2$$<br>And if we vectorise it further, we’ll get<br>$$<br>L &#x3D; \frac{1}{N}(t-Xw)^{T}(t-Xw)<br>$$<br>X indicates the data matrix we have mentioned below and w indicates the parameter list to be optimised(a N-dimensional array in this case)</li>
</ol>
<p>so we can vectorise it further, something like below:</p>
<ol start="3">
<li>optimise loss - Take partial deriaitve of loss w.r.t each parameter</li>
<li>Set to zero and solve(generally, we have K simultaneous equations)<br>$$\frac{\partial L}{\partial w}$$<br> note that in the duduction process, we can combine (W^{T}X^{T}t) and (t^{T}Xw)<br> we finally get:<br>$$<br>w &#x3D; (X^{T}X)^{-1}X^{T}t<br>$$</li>
<li>make prediction and evaluation<br>To predict t at a new value of x, we first create:<br>$$<br>X_{new}&#x3D;\begin{bmatrix}<br>h_{0}(x_{new})<br>\newline<br>\vdots<br>\newline<br>h_{K}(x_{new})<br>\end{bmatrix}<br>$$</li>
</ol>
<p>and then compute:<br>$$<br>t_{new} &#x3D; \hat{W}^{T}X_{new}<br>$$<br>and then compute test loss.</p>
<h4 id="2-2-Common-basis-functions-h-x-something-like-activation-function-in-DL-i-guess"><a href="#2-2-Common-basis-functions-h-x-something-like-activation-function-in-DL-i-guess" class="headerlink" title="2.2 Common basis functions h(x) - something like activation function in DL(i guess)"></a>2.2 Common basis functions h(x) - something like activation function in DL(i guess)</h4><ol>
<li>Radial basis function(RBF) - Gaussain Basis?<br>$$<br>h_{k}(x) &#x3D; exp(-\frac{(x-μ_{k})^{2}}{2s^{2}})<br>$$</li>
<li>Sigmoid function<br>$$<br>h_{k}(x) &#x3D; \sigma(\frac{(x-\mu)^{2}}{s})<br>$$</li>
</ol>
<p>$$<br>\sigma(a)&#x3D;\frac{1}{1+exp(-a)}<br>$$<br>3. polynomial form we have mentioned above</p>
<p>From my perspective, personally, i think we can get more flexible representation of data by using those transformations. </p>
<h4 id="2-3-Generalisation-and-Overfitting"><a href="#2-3-Generalisation-and-Overfitting" class="headerlink" title="2.3 Generalisation and Overfitting"></a>2.3 Generalisation and Overfitting</h4><p>To some extent, generalisation reflects the performance of our model<br>There is a trade-off between the generalisation and over-fittintg(increase order in the polynomial regression to decrease the loss)</p>
<p><strong>Cross Vilidation for evaluating the performance of our model</strong><br> use <strong>Fold1 - FoldC-1</strong> to train our models(training data)<br> use <strong>remaining FoldC</strong> to test our prediction of models(validation data)</p>
<p><strong>Leave-one-out Cross-Validation - LOOCV</strong><br>something like Do n-fold training for n times.</p>
<h3 id="3-Bayesian-Regression"><a href="#3-Bayesian-Regression" class="headerlink" title="3.Bayesian Regression"></a>3.Bayesian Regression</h3><p>we have seen two ways of finding best parameter values:</p>
<ol>
<li>Those that minimise the loss</li>
<li>Those that maximise the likelihood.</li>
<li>note that if the noise is guassian in probability model, both are the same<br> that is:<br> by using:<br>$$<br>\frac{\partial L}{\partial w}<br>$$<br> we get:<br>$$<br>\hat{w}&#x3D;(X^{T}X)^{-1}X^{T}t<br>$$</li>
</ol>
<p><strong>hope there will be update about formula deduction by the author.</strong></p>
<h4 id="3-1-problems-with-a-point-estimate"><a href="#3-1-problems-with-a-point-estimate" class="headerlink" title="3.1 problems with a point estimate"></a>3.1 problems with a point estimate</h4><ol>
<li>might be more than one best value</li>
<li>Different values might have different performance of prediction</li>
</ol>
<p>What will our prediction be like if we use the weight method?</p>
<ol>
<li>we know that prediction is some function form of paramter ‘w’(i.e. f(w))</li>
<li>Choose A different representations of ‘w’ set - w from 1 to A.</li>
<li>then compute:(qa is proportional to L)<br>$$<br>\sum_{a&#x3D;1}^{A}q_{a}f(w_{a})<br>$$<br> and we know that:<br>$$<br>\sum_{a}q_{a} &#x3D; 1<br>$$</li>
</ol>
<h4 id="3-2-something-about-w’s-density-and-expectation"><a href="#3-2-something-about-w’s-density-and-expectation" class="headerlink" title="3.2 something about w’s density and expectation"></a>3.2 something about w’s density and expectation</h4><p>by using expectation below:<br>$$<br>E_{p(w|stuff)}{f(w)} &#x3D; \int f(w)p(w|stuff) dw<br>$$<br>An average of predictions from each possible <strong>w</strong> weighted by how likely that <strong>w</strong> value is.<br><strong>I should say this expression on the slides is so annoying, lol</strong><br>my version: to what extent, we can consider our prediction to be tnew.</p>
<p>Never mind, more importantly</p>
<ol>
<li>What exactly is the <strong>‘stuff’</strong>?</li>
<li>How can we calculate<br>$$<br>p(w|stuff)<br>$$</li>
</ol>
<h4 id="3-3-Bayes-Rules"><a href="#3-3-Bayes-Rules" class="headerlink" title="3.3 Bayes Rules"></a>3.3 Bayes Rules</h4><p>seems that it’s wise for us to include some information about we know after observing the data set in our ‘stuff’ case.<br>that is:<br>$$<br>p(w|X,t)<br>$$</p>
<p>And establishing the model(like simply fitting a line in regression) in this way is to use <strong>p(t|X,w)</strong> to find <strong>p(w|X,t)</strong></p>
<p>It reminds me of my first research meeting about Variational Bayes. I should say it’s a terrible experience.</p>
<p>Anyway, by using <strong>Bayes Rules</strong>:<br>$$<br>p(w|X,t) &#x3D; \frac{p(t|X,w)p(w)}{p(t|X)}<br>$$<br>and we call it posterior density.</p>
<p>Likelihood:<br>$$<br>p(t|X,w)<br>$$</p>
<p>Prior density:<br>$$<br>p(w)<br>$$<br>Marginal likelihood:<br>$$<br>p(t|X)<br>$$<br>and<br>$$<br>\int p(w|X,t) dw &#x3D; 1<br>$$</p>
<p>so i would like to give a brief deduction about the <strong>‘posterior density’</strong><br>To prove<br>$$<br>p(w|X,t) &#x3D; \frac{p(t|X,w)p(w)}{p(t|X)}<br>$$</p>
<p>we first prove<br>$$<br>p(w,t|X) &#x3D; p(t|w,X)p(w|X)<br>$$<br>Any by applying Bayes Rules:<br>$$<br>p(w,t|X) &#x3D; \frac{p(t|w,X)p(w,X)}{(X)} &#x3D; p(t|w,X)p(w|X)&#x3D;p(t|w,X)\frac{p(X|w)p(w)}{p(X)}<br>$$<br>we now get the <strong>‘p(w)’</strong><br>Rearrange and combine what we have gotten previously:<br>$$<br>p(w|X,t) &#x3D; \frac{p(w,t|X)p(X)}{p(X,t)} &#x3D; \frac{p(w,t|X)}{p(t|X)}<br>$$<br>we now get the <strong>‘p(t|X)’</strong> in the denominator<br>$$<br>p(w|X,t) &#x3D; \frac{p(t|w,X)p(X|w)p(w)}{p(t|X)p(X)}<br>$$<br>we know the data ‘X’ represents constant set, so <strong>p(X)</strong> and <strong>p(X|w)</strong> are both euqal to <strong>1</strong>.<br>finally, we got it!</p>
<h4 id="3-4-Bayesian-machine-learning"><a href="#3-4-Bayesian-machine-learning" class="headerlink" title="3.4 Bayesian machine learning"></a>3.4 Bayesian machine learning</h4><ol>
<li>binary classifier for spam email in a bayesian ML vision</li>
<li>computing the posterior is really a challenge<br>cas we have difficulty calculating the marginal likelihood of <strong>‘p(t|X)’</strong><br>$$<br>p(t|X) &#x3D; \int p(t|w,X)p(w) dw<br>$$<br>so in most cases, we’re forced to:</li>
<li>approximate p(w|X,t) with something else</li>
<li>Sample from p(w|X,t)</li>
</ol>
<p>so we may use some tricks to approximate our <strong>‘p(w|X,t)’</strong></p>
<p><strong>Conjugacy</strong><br>find more detailed information <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Conjugate_prior">here</a></p>
<h2 id="Unit2-Classification"><a href="#Unit2-Classification" class="headerlink" title="Unit2 Classification"></a>Unit2 Classification</h2><p>4 classification algorithms - probabilistic vs. non-probabilistic<br>our probabilistic classifiers are expected to produce series of class membership<br>$$<br>P(t_{new}&#x3D;k|x_{new}, X, t)<br>$$<br>non-probabilistic classifiers directly assign a single class label to an input label,without any associated probability or confidence score</p>
<p>probabilities provide us more information<br>\begin{align}P(t_{\mathrm{new}}&#x3D;1)\stackrel{.}{&#x3D;}0.6\end{align} is more useful than \begin{align}\mathrm{than}\ t_{n e w}&#x3D;1\end{align}</p>
<h3 id="1-1-K-Nearest-Neighbours"><a href="#1-1-K-Nearest-Neighbours" class="headerlink" title="1.1 K-Nearest Neighbours"></a>1.1 K-Nearest Neighbours</h3><ol>
<li><p>Non-probabilistic </p>
</li>
<li><p>multi-class</p>
</li>
<li><p>no ‘training’ phase and only one parameter to tune(K)  </p>
</li>
<li><p>workflow:</p>
</li>
<li><p>choose K</p>
</li>
<li><p>for every test object Xnew</p>
</li>
<li><p>find K closest points from the training set</p>
</li>
<li><p>find majority class with in the K ‘neighbours’</p>
</li>
<li><p>assign the label</p>
</li>
<li><p>problems with KNN - class imbalance<br> Suppose we have 100 class-1 data, and 2 class-2 data in a dataset.<br> As our K increases, our model will be more likely to assign each Xnew with the label ‘class-1’</p>
</li>
<li><p>Cross-validation in classification</p>
</li>
<li><p>split the data up - some for training, some for test</p>
</li>
<li><p>figure out the number of mis-classification over the C-fold</p>
</li>
</ol>
<h3 id="1-2-Naive-Bayes-Classifier"><a href="#1-2-Naive-Bayes-Classifier" class="headerlink" title="1.2 Naive Bayes Classifier"></a>1.2 Naive Bayes Classifier</h3><p>our probabilistic classifier is based on Bayes rule:<br>$$<br>P(t_{new}&#x3D;k|X,t,x_{new}) &#x3D; \frac{P(x_{new}|t_{new}&#x3D;k,X,t)P(t_{new}&#x3D;k)}{\sum_{j}p(x_{new}|t_{new}&#x3D;j,X,t)P(t_{new}&#x3D;j)}<br>$$</p>
<p>Naive-Bayes makes the following additional likelihood assumption:</p>
<p>FUck glasgow</p>
<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/09/20/Learning-ML-from-scratch/" data-id="clp635lp30000bgfw4bfx5whg" data-title="Learning ML from scratch" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/26/learning-from-scratch-2023-09-26/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          learning from scratch-2023.09.26
        
      </div>
    </a>
  
  
    <a href="/2023/09/15/ML-learning-from-scratch/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">ML learning from scratch</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Programming-2023tutorial/">-Programming -2023tutorial</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/c-Programming/" rel="tag">-c++ -Programming</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/deep-learning/" rel="tag">-deep learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm-Leetcode-Graph-Problems/" rel="tag">Algorithm, Leetcode, Graph Problems</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/COMPSCI4084/" rel="tag">COMPSCI4084</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Introduction-to-data-science-and-system/" rel="tag">Introduction to data science and system</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/learning-from-scratch/" rel="tag">learning from scratch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/" rel="tag">machine learning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/c-Programming/" style="font-size: 10px;">-c++ -Programming</a> <a href="/tags/deep-learning/" style="font-size: 10px;">-deep learning</a> <a href="/tags/Algorithm-Leetcode-Graph-Problems/" style="font-size: 10px;">Algorithm, Leetcode, Graph Problems</a> <a href="/tags/COMPSCI4084/" style="font-size: 10px;">COMPSCI4084</a> <a href="/tags/Introduction-to-data-science-and-system/" style="font-size: 10px;">Introduction to data science and system</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/learning-from-scratch/" style="font-size: 20px;">learning from scratch</a> <a href="/tags/machine-learning/" style="font-size: 15px;">machine learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">September 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">February 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/11/19/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2023/11/19/notice-for-5089-past-exam-paper/">notice for 5089 past exam paper</a>
          </li>
        
          <li>
            <a href="/2023/10/24/notice-for-idss-week5-optimization2/">notice for idss week5 optimization2</a>
          </li>
        
          <li>
            <a href="/2023/10/22/notice-for-idss-week4-optimization/">notice for idss week4 - optimization</a>
          </li>
        
          <li>
            <a href="/2023/10/20/notice-for-java-assessment/">notice for java assessment</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 Joseph Wang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>